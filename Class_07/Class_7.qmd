---
title: "Class 7: Machine Learning 1"
author: "Abel (PID 59018056)"
format: pdf
---

Before we get into clustering methods let's make some sample data to cluster where we know what the aswer should be.

To help with this, I will use the `rnomrm()` function

```{r}
hist(rnorm(150000, mean = 3))
```

```{r}
n = 10000
hist(c(rnorm(n, mean = 3), rnorm(n, mean=-3)))
```

```{r}
n = 30 
x <- c(rnorm(n, mean = 3), rnorm(n, mean=-3))
y <- rev(x)


z <- cbind(x, y)
plot(z)
```

## K-mean clustering

The function in base R for K-means clustering is called `kmeans()`.

```{r}
km <- kmeans(z, centers =2)
km
```

```{r}
km$centers

```

> Q. Print out the cluster membership vectro (i.e our main answer)

```{r}
km$cluster
```

```{r}
plot(z, col = c("red", "blue"))
```

```{r}
plot(x, col = c(1,2))
```

Plot with clustering result and add cluster centers:

```{r}
plot(z, col = km$cluster)
points(km$centers, col = "blue", pch = 17, cex = 2)
```

\# phc = different shapes \# cex = character ambelishment

> Q. Can you cluster our data in `z` into four cluster please

```{r}
km4 <- kmeans(z, centers = 4)
plot(z, col = km4$cluster)
points(km4$centers, col = "blue", pch = 15, cex = 2)
```

## Hierarchal Clustering

The manin function for hierarchal clustering is `hclust()`

Unlike `kmeans()` I cannot just pass in my data as input. I first need a distance matrix from my data

```{r}
d <- dist(z)
hc <- hclust(d)
hc 
```

There is a specific hclust plot() method...

```{r}
plot(hc)
abline(h = 10, col = "red")
```

To get my main clusterign result (i.e the membership vector) I can "cut" my tree at a given height. To do this I will use the `cutreee()`

```{r}
grps <- cutree(hc, h = 10)
grps
```

```{r}
plot(z, col = grps, pch = 1, cex = 2)

```




```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
nrow(x)
ncol(x)
 ## x <- x[,-1]

##head(x)
```


```{r}
x <- read.csv(url, row.names=1)
head(x)
```


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
```{r}
pairs(x, col=rainbow(10), pch=16)
```
## Principal Component Analysis (PCA)

The main function to do PCA in base R is called `prcomp()`.

Note that I need to take the transpose of this particlular data as that is what the `prcomp()` help page was asking for.
```{r}
pca <- prcomp(t(x))
summary(pca)
```

Let's see what is inside our result object `pca` that wwe just calulated 

```{r}
attributes(pca)
```
```{r}
pca$x
```
To make our main result figure, called a "PC plot" (or "score plot", "ordination plot", or "PC1 vs PC2 plot").

```{r}
plot(pca$x[,1], pca$x[,2], col = c("black", "red", "blue", "darkblue"), pch = 16, xlab = "PC1(67.4%", ylab = "PC2 (29%")
abline(h = 0, col = "gray", lty = 2)
abline(v = 0, col = "gray", lty = 2)
```
## Variable Loading plot 

Can give us insight on how the original variable

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

